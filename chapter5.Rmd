# Chapter 5: Dimensionality reduction techniques
***
In this chapter, we learn the basics of two data science based ways of reducing the dimensions. The key method is principal component analysis (PCA), which reduces any number of measured (continuous) and correlated variables into a few uncorrelated components that collect together as much variance as possible from the original variables. The most important components can be then used for various purposes, e.g., drawing scatterplots and other fancy graphs that would be quite impossible to achieve with the original variables and too many dimensions. 

***

> **Step 1.** Loading and checking the data. 

The `human` data originates from the United Nations Development Programme.The Human Development Index (HDI) is a summary measure of average achievement in key dimensions of human development: a long and healthy life, being knowledgeable and have a decent standard of living. The HDI is the geometric mean of normalized indices for each of the three dimensions. The data contains 155 observations and 8 variables with the country as the `rolenames`. Detailed explanation of the variables can be found [here](http://hdr.undp.org/en/content/human-development-index-hdi). All the variables are either numeric or integerial (`dni` was changed from factor variable to numeric variable). 

```{r}
human <- read.table("Data/human.txt")
human <- within(human, {
  gni <- as.character(gni)
})
human <- within(human, {
  gni <- as.numeric(sub(",", "", gni, fixed = TRUE))
})
str(human)
dim(human)
```

A graphical overview, a summary and a correlation matrix were plotted for `human` data and shown below. You can find the distribution of the data in the summary table with some statistical measures. 
```{r}
library(corrplot)
library(tidyverse)
library(dplyr)
library(tidyr)
library(GGally)
library(ggplot2)
pairs(human)
summary(human)
```

The correlation matrix shown here are in the form of a heat-map. Dark red color meaning negatively correlated, dark blue color meaning positively correlated, and white meaning no correlation. You can easily spot certain pairs that have stronger correlations from the graph.  
```{r}
cor(human) %>% corrplot(method="color", order="hclust", type = "upper", cl.pos = "r", tl.pos = "d", tl.cex = 0.6, title = "Correlation plots for human", mar=c(0,0,1,0))
```

> **Step 2.** Perform principal component analysis (PCA) on the not standardized human data. 

The requested PCA was performed on the non-standardized human data. 
```{r}
pca_human <- prcomp(human)
s <- summary(pca_human)
s
```

We can see from the summary of the PCA that the `gni` variable captured 100% of the variability. This is of course not correct. Why? Simply because the values are too huge compared to all other variables. Here the biplot is drawn and you can see that the arrow of `gni` is parallel to the PC1, meaning that they are basially the same determining factor during the analysis. 
```{r}
pca_pr <- round(100*s$importance[2, ], digits = 1)
pca_pr
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```

> **Step 3.** Perform principal component analysis (PCA) on the standardized human data.

To overcome the problem with regards to PCA analysis. The `human` data is first standardized. 
```{r}
human_std <- scale(human)
summary(human_std)
```

The PCA is again run on the standardized data: 
```{r}
pca_human_std <- prcomp(human_std)
s2 <- summary(pca_human_std)
s2
```
Now we can see that the PC 1 captured 53.6% of variance and PC 2 16.2%. The biplot is again drawn. 

```{r}
pca_pr_std <- round(100*s2$importance[2, ], digits = 1)
pca_pr_std
pc_lab_std <- paste0(names(pca_pr), " (", pca_pr_std, "%)")
biplot(pca_human_std, cex = c(0.5, 0.8), col = c("grey40", "deeppink2"), xlab = pc_lab_std[1], ylab = pc_lab_std[2])
```

From the biplot, we can see that actually many variables are almost parallel to PC 1, including `edutime`, `eduRatio`, `teenbirth`, `dni`, `mortalityM` and `lifebirth`. The other two variables `ParF` and `labRatio` are almost paraelle to PC 2. We see that `dni` does not have determining power over all the data thanks to the standardization process.  

> **Step 3.** Analysis of TEA data. 

Loading the dataset from the package `FactoMineR` and explore the data briefly. The data has 300 observations and 36 variables. Most of the variables are factor variables with only the `age` variable as numeric. 

```{r}
library(FactoMineR)
data(tea)
TEA_ <- tea
str(TEA_)
dim(TEA_)
```

Here, we select a subset of the `tea` data set for analysis. 
```{r}
kc <- c("Tea", "escape.exoticism", "spirituality", "healthy", "diuretic", "friendliness", "sophisticated", "slimming", "exciting", "relaxing", "effect.on.health")
tea_type_reason <- dplyr::select(TEA_, one_of(kc))
summary(tea_type_reason)
str(tea_type_reason)
```

The selected data set is based on different tea types and different reasons for drinking tea. It is visualized here by bar plots. 

```{r}
gather(tea_type_reason) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5))
```

The MCA of the selected data is conducted. The summary of the MCA is printed here:

```{r}
mca <- MCA(tea_type_reason, graph = FALSE)
summary(mca)
```

Overall, 12 dimensions are need to fully cover the variability of the variables. We can see that none of the variables contributed to more than 0.5 squared correlation to any computed dimension. From the v.test values, we can see that if the absolute value of the v.test is larger than 1.96, then the coordinate is significately different from 0. This means that `Dim.1` is strongly related to the attitude of why people are drinking tea, but not to the type of the tea. 

Next, the variable biplot of the MCA is drawn. 
```{r}
plot(mca, invisible=c("ind"), habillage = "quali")
```

It seems that nothing really stand out in this MCA analysis. All dimensions captured similar amount of variablility and the most significant dimensions `Dim.1` and `Dim.2` is not strongly correlated with any of the variables. 

> **Concluding remarks.** 

In this chapter, PCA and MCA analysis were covered. For data including multiple variables, it is often useful to reduce the variable dimensions so that the data can be plot and visualized easilier. The most useful methods are PCA and MCA, both of which effectively reduce the dimension. 